missing_cols <- setdiff(common_cols, colnames(kaggle_x_mat))
if(length(missing_cols)) {
add_mat <- matrix(0, nrow = nrow(kaggle_x_mat), ncol = length(missing_cols),
dimnames = list(NULL, missing_cols))
kaggle_x_mat <- cbind(kaggle_x_mat, add_mat)
}
}
kaggle_x_mat <- kaggle_x_mat[, common_cols, drop = FALSE]
kaggle_raw <-dataAREG_test_final
# Preparar kaggle dataset (eliminar ID primero, lo guardamos aparte)
kaggle_raw <- data_imputed_AREG_test %>% dplyr::select(-Surname)
# Preparar kaggle dataset (eliminar ID primero, lo guardamos aparte)
kaggle_raw <- dataAREG_test_final %>% dplyr::select(-Surname)
kaggle_ids <- kaggle_raw$ID
kaggle_raw <- kaggle_raw %>% dplyr::select(-ID)
kaggle_raw <- apply_feature_engineering(kaggle_raw, is_train = FALSE)
kaggle_x_mat <- predict(dv, newdata = kaggle_raw) %>% as.matrix()
###############################################################################
# 8) SUBMISSION PARA KAGGLE (usamos el meta-modelo + umbral óptimo del meta)
###############################################################################
# NB
nb_prob_kaggle <- predict(modelo_nb_final, newdata = kaggle_raw, type = "prob")[, "Yes"]
# LASSO y LGBM - alinear columnas
common_cols <- colnames(x_train_mat)
if(!all(common_cols %in% colnames(kaggle_x_mat))) {
missing_cols <- setdiff(common_cols, colnames(kaggle_x_mat))
if(length(missing_cols)) {
add_mat <- matrix(0, nrow = nrow(kaggle_x_mat), ncol = length(missing_cols),
dimnames = list(NULL, missing_cols))
kaggle_x_mat <- cbind(kaggle_x_mat, add_mat)
}
}
kaggle_x_mat <- kaggle_x_mat[, common_cols, drop = FALSE]
lasso_prob_kaggle <- predict(cv_lasso, newx = Matrix::Matrix(kaggle_x_mat, sparse = TRUE),
s = "lambda.min", type = "response")[,1]
lgb_prob_kaggle <- predict(lgb_model_final, kaggle_x_mat)
meta_input_kaggle <- data.frame(
nb_prob = nb_prob_kaggle,
lasso_prob = lasso_prob_kaggle,
lgb_prob = lgb_prob_kaggle
)
meta_prob_kaggle <- predict(meta_glm, newdata = meta_input_kaggle, type = "response")
# Aplicar el umbral óptimo del meta
best_meta_thr <- thr_meta$best_threshold
pred_kaggle <- ifelse(meta_prob_kaggle > best_meta_thr, "Yes", "No")
submission <- data.frame(
ID = kaggle_ids,
Exited = pred_kaggle
)
# Guardar CSV
write.csv(submission, file = "submission_meta_model_FE.csv", row.names = FALSE)
cat("\nSubmission saved to submission_meta_model_FE.csv (threshold meta:", best_meta_thr, ")\n")
cat("Features totales creadas:", ncol(x_train_mat), "\n")
data <- dataAREG_final[,-c(1,2)]
Index <- sample(1:nrow(data), size = nrow(data)*0.8)
dataTrain <- data[Index, ]
dataTest  <- data[-Index, ]
# Filtrar las variables seleccionadas del conjunto de datos
dataTrain_subset <- dataTrain[, c("Exited", seleccio)]
dataTrain$Exited <- factor(dataTrain$Exited, levels = c(0,1), labels = c("No", "Yes"))
dataTest$Exited  <- factor(dataTest$Exited,  levels = c(0,1), labels = c("No", "Yes"))
# F1
f1 <- function(data, lev = NULL, model = NULL) {
f1_val <- MLmetrics::F1_Score(y_pred = data$pred, y_true = data$obs, positive = "Yes")
c(F1 = f1_val)
}
control <- trainControl(
method = "repeatedcv",
number = 10,
repeats = 10,
classProbs = TRUE,
summaryFunction = f1,
sampling = "smote",
verboseIter = TRUE
)
# Grid inicial
grid <- expand.grid(
usekernel = c(TRUE, FALSE),
laplace = c(0, 1, 2),
adjust = c(0, 1, 2)
)
# Entrenamiento inicial
modelo_nb <- train(
Exited ~ .,
data = dataTrain,
method = "naive_bayes",
trControl = control,
tuneGrid = grid,
metric = "F1"
)
print(modelo_nb)
# Predicciones (selecciona automaticamente los mejores parametros)
pred_class <- predict(modelo_nb, newdata = dataTest, type = "raw")
pred_prob  <- predict(modelo_nb, newdata = dataTest, type = "prob")
cm <- confusionMatrix(pred_class, dataTest$Exited, positive = "Yes")
print(cm)
# Esto es una mierda, se puede hacer tb tuning del llindar
probs <- predict(modelo_nb2, newdata = dataTest, type = "prob")[, "Yes"]
preds <- ifelse(probs > 0.2, "Yes", "No")
# Esto es una mierda, se puede hacer tb tuning del llindar
probs <- predict(modelo_nb, newdata = dataTest, type = "prob")[, "Yes"]
preds <- ifelse(probs > 0.2, "Yes", "No")
confusionMatrix(factor(preds, levels=c("No","Yes")), dataTest$Exited, positive="Yes")
thresholds <- seq(0.01, 0.8, by = 0.01)
# Inicializar un vector para almacenar los resultados del F1
f1_scores <- numeric(length(thresholds))
# Loop para probar cada threshold y calcular el F1
for (i in seq_along(thresholds)) {
threshold <- thresholds[i]
preds <- ifelse(probs > threshold, "Yes", "No")
f1_scores[i] <- MLmetrics::F1_Score(y_pred = preds, y_true = dataTest$Exited, positive = "Yes")
}
threshold_f1_df <- data.frame(Threshold = thresholds, F1_Score = f1_scores)
print(threshold_f1_df)
best_threshold <- threshold_f1_df$Threshold[which.max(threshold_f1_df$F1_Score)]
best_threshold
preds_testtrain <- ifelse(probs > best_threshold, "Yes", "No")
# CM
cm_best <- confusionMatrix(
factor(preds_testtrain, levels = c("No", "Yes")),
dataTest$Exited,
positive = "Yes"
); print(cm_best)
MLmetrics::F1_Score(dataTest$Exited,preds_testtrain,"Yes")
# 1. Obtener probabilidades de test
probs_test <- predict(modelo_nb2, newdata = data_imputed_AREG_test[-6], type = "prob")[, "Yes"]
# 1. Obtener probabilidades de test
probs_test <- predict(modelo_nb2, newdata = dataAREG_test_final, type = "prob")[, "Yes"]
# 1. Obtener probabilidades de test
probs_test <- predict(modelo_nb, newdata = dataAREG_test_final, type = "prob")[, "Yes"]
# 2. Aplicar el mejor threshold encontrado
best_threshold <- threshold_f1_df$Threshold[which.max(threshold_f1_df$F1_Score)]
pred_test <- ifelse(probs_test > best_threshold, "Yes", "No")
# 3. Crear el dataframe de submission
submission <- data.frame(
ID = data_imputed_AREG_test$ID,
Exited = pred_test
)
# 3. Crear el dataframe de submission
submission <- data.frame(
ID = dataAREG_test_final$ID,
Exited = pred_test
)
# 4. Guardar el CSV
write.csv(submission, "submission_nb_Smote.csv", row.names = FALSE)
data = dataAREG_final[,-c(1,2)]
vars = colnames(data)[-21]
par(mfrow = c(4,5), mar = c(3,3,3,1))
for (va in vars){
if (!is.factor(data[,va])){
boxplot(as.formula(paste0(va,"~Exited")),data,main=va,col=c(2,3),horizontal=T)
} else{
barplot(prop.table(table(data$Exited, data[,va]),2),main=va,col=c(2,3))
}
}
set.seed(123)
Index <- sample(1:nrow(data), size = nrow(data)*0.8)
dataTrain <- data[Index, ]
dataTest  <- data[-Index, ]
dataTrain$Exited <- factor(dataTrain$Exited, levels = c(0,1), labels = c("No", "Yes"))
dataTest$Exited  <- factor(dataTest$Exited,  levels = c(0,1), labels = c("No", "Yes"))
# F1
f1 <- function(data, lev = NULL, model = NULL) {
f1_val <- MLmetrics::F1_Score(y_pred = data$pred, y_true = data$obs, positive = "Yes")
c(F1 = f1_val)
}
mod1 = glm(Exited ~ ., family = binomial(link="logit"),dataTrain)
mod2 = glm(Exited ~ ., family = binomial(link="probit"),dataTrain)
mod3 = glm(Exited ~ ., family = binomial(link="cloglog"),dataTrain)
deviance(mod1);deviance(mod2);deviance(mod3)
AIC(mod1);AIC(mod2);AIC(mod3)
# ES MILLOR LINK LOGIT
par(mfrow=c(2,2))
plot(mod1)
prob1 = predict(mod1, newdata = dataTest, type="response")
pred1 = ifelse( prob1 > 0.2,"Yes","No")
MLmetrics::F1_Score(dataTest$Exited,pred1)
MLmetrics::Sensitivity(dataTest$Exited,pred1)
data =scal(data)
data =scale(data)
library(dplyr)
data <- data %>% mutate(across(where(is.numeric), scale))
vars = colnames(data)[-21]
set.seed(123)
Index <- sample(1:nrow(data), size = nrow(data)*0.8)
dataTrain <- data[Index, ]
dataTest  <- data[-Index, ]
dataTrain$Exited <- factor(dataTrain$Exited, levels = c(0,1), labels = c("No", "Yes"))
dataTest$Exited  <- factor(dataTest$Exited,  levels = c(0,1), labels = c("No", "Yes"))
# F1
f1 <- function(data, lev = NULL, model = NULL) {
f1_val <- MLmetrics::F1_Score(y_pred = data$pred, y_true = data$obs, positive = "Yes")
c(F1 = f1_val)
}
############################ Model inicial #####################################
mod1 = glm(Exited ~ ., family = binomial(link="logit"),dataTrain)
prob1 = predict(mod1, newdata = dataTest, type="response")
pred1 = ifelse( prob1 > 0.2,"Yes","No")
MLmetrics::F1_Score(dataTest$Exited,pred1)
MLmetrics::Sensitivity(dataTest$Exited,pred1)
pred1 = ifelse( prob1 > 0.5,"Yes","No")
MLmetrics::F1_Score(dataTest$Exited,pred1)
MLmetrics::Sensitivity(dataTest$Exited,pred1)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(NetPromoterScore, df = 3) +
ns(TransactionFrequency, df = 3) +
ns(AvgTransactionAmount, df = 3) +
ns(DigitalEngagementScore, df = 3) +
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + EducationLevel + LoanStatus + Geography +
ComplaintsCount + HasCrCard + IsActiveMember +
CustomerSegment + MaritalStatus + SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
############################ Modelatge amb splines #############################
library(splines)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(NetPromoterScore, df = 3) +
ns(TransactionFrequency, df = 3) +
ns(AvgTransactionAmount, df = 3) +
ns(DigitalEngagementScore, df = 3) +
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + EducationLevel + LoanStatus + Geography +
ComplaintsCount + HasCrCard + IsActiveMember +
CustomerSegment + MaritalStatus + SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(TransactionFrequency, df = 3) +
ns(AvgTransactionAmount, df = 3) +
ns(DigitalEngagementScore, df = 3) +
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + EducationLevel + LoanStatus + Geography +
ComplaintsCount + HasCrCard + IsActiveMember +
CustomerSegment + MaritalStatus + SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(AvgTransactionAmount, df = 3) +
ns(DigitalEngagementScore, df = 3) +
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + EducationLevel + LoanStatus + Geography +
ComplaintsCount + HasCrCard + IsActiveMember +
CustomerSegment + MaritalStatus + SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(DigitalEngagementScore, df = 3) +
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + EducationLevel + LoanStatus + Geography +
ComplaintsCount + HasCrCard + IsActiveMember +
CustomerSegment + MaritalStatus + SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + EducationLevel + LoanStatus + Geography +
ComplaintsCount + HasCrCard + IsActiveMember +
CustomerSegment + MaritalStatus + SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + LoanStatus + Geography +
ComplaintsCount + HasCrCard + IsActiveMember +
CustomerSegment + MaritalStatus + SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + Geography +
ComplaintsCount + HasCrCard + IsActiveMember +
CustomerSegment + MaritalStatus + SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + Geography +
HasCrCard + IsActiveMember +
CustomerSegment + MaritalStatus + SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + Geography +
HasCrCard + IsActiveMember +
MaritalStatus + SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + Geography +
HasCrCard + IsActiveMember +
SavingsAccountFlag +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
mod_splines <- glm(
Exited ~
# Variables continuas con splines (3-5 grados de libertad)
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + Geography +
HasCrCard + IsActiveMember +
NumOfProducts,
family = binomial(link = "logit"),
data = dataTrain
)
prob2 = predict(mod_splines, newdata = dataTest, type="response")
pred2 = ifelse( prob2 > 0.5,"Yes","No")
MLmetrics::F1_Score(dataTest$Exited,pred2)
MLmetrics::Sensitivity(dataTest$Exited,pred2)
plot(mod_splines)
summary(mod_splines)
AIC(mod1);AIC(mod_splines)
ConfusionMatrix(pred2,dataTest$Exited)
library(MLmetrics)
library(dplyr)
library(glmnet)
library(caret)
library(doParallel)
library(splines)
ConfusionMatrix(pred2,dataTest$Exited)
# Fórmula con splines
formula_completa <- Exited ~
ns(Age, df = 4) +                    # Relación en U con edad
ns(CreditScore, df = 3) +
ns(Balance, df = 2) +                # Balance puede tener efectos no lineales
ns(Tenure, df = 3) +                 # Tenure al cuadrado típicamente
ns(EstimatedSalary,df=3) +                    # Linear si no hay evidencia
# Variables categóricas
Gender + Geography +
HasCrCard + IsActiveMember +
NumOfProducts
# TODAS las interacciones de orden 2
formula_interacciones <- update(formula_completa, ~ (.)^2)
# Configuración CV optimizando F1
train_control_f1 <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = f1,
verboseIter = TRUE,
allowParallel = TRUE,
sampling = "smote"
)
# Grid de hiperparámetros
elastic_grid <- expand.grid(
alpha = seq(0, 1, by = 0.2),
lambda = exp(seq(-4, 0, length.out = 10))
)
elastic_f1 <- train(
formula_interacciones,
data = dataTrain,
method = "glmnet",
trControl = train_control_f1,
tuneGrid = elastic_grid,
metric = "F1",                    # OPTIMIZAR F1
family = "binomial"
)
prob3 = predict(elastic_f1, newdata = dataTest, type="prob")
pred3 = ifelse( prob3 > 0.25,"Yes","No")[,2]
MLmetrics::F1_Score(dataTest$Exited,pred3)
MLmetrics::Specificity(dataTest$Exited,pred3)
pred3 = ifelse( prob3 > 0.5,"Yes","No")[,2]
MLmetrics::F1_Score(dataTest$Exited,pred3)
MLmetrics::Specificity(dataTest$Exited,pred3)
thresholds <- seq(0.01, 0.8, by = 0.01)
# Inicializar un vector para almacenar los resultados del F1
f1_scores <- numeric(length(thresholds))
# Loop para probar cada threshold y calcular el F1
for (i in seq_along(thresholds)) {
threshold <- thresholds[i]
preds <- ifelse(prob3 > threshold, "Yes", "No")
f1_scores[i] <- MLmetrics::F1_Score(y_pred = preds[,2], y_true = dataTest$Exited, positive = "Yes")
}
threshold_f1_df <- data.frame(Threshold = thresholds, F1_Score = f1_scores)
print(threshold_f1_df)
best_threshold <- threshold_f1_df$Threshold[which.max(threshold_f1_df$F1_Score)]
best_threshold
preds_testtrain <- ifelse(prob3 > best_threshold, "Yes", "No")[,2]
# CM
cm_best <- confusionMatrix(
factor(preds_testtrain, levels = c("No", "Yes")),
dataTest$Exited,
positive = "Yes"
); print(cm_best)
# CM
cm_best <- confusionMatrix(
factor(preds_testtrain, levels = c("No", "Yes")),
dataTest$Exited,
positive = "Yes"
); print(cm_best)
MLmetrics::F1_Score(dataTest$Exited,preds_testtrain,"Yes")
# 1. Obtener probabilidades de test
probs_test <- predict(elastic_f1, newdata = dataAREG_test_final[-6], type = "prob")[, "Yes"]
# 1. Obtener probabilidades de test
probs_test <- predict(elastic_f1, newdata = dataAREG_test_final, type = "prob")[, "Yes"]
pred_test <- ifelse(probs_test > 0.25, "Yes", "No")
print(threshold_f1_df)
best_threshold <- threshold_f1_df$Threshold[which.max(threshold_f1_df$F1_Score)]
best_threshold
pred_test <- ifelse(probs_test > 0.57, "Yes", "No")
# 3. Crear el dataframe de submission
submission <- data.frame(
ID = dataAREG_test_final$ID,
Exited = pred_test
)
# 4. Guardar el CSV
write.csv(submission, "submission_glm.csv", row.names = FALSE)
